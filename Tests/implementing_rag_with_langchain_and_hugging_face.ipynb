{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q langchain\n",
    "!pip install -q torch\n",
    "!pip install -q transformers\n",
    "!pip install -q sentence-transformers\n",
    "!pip install -q datasets\n",
    "!pip install -q faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import HuggingFaceDatasetLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "from langchain_text_splitters import RecursiveJsonSplitter\n",
    "import json\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.document import Document\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on an other text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\datasets\\load.py:2554: FutureWarning: 'use_auth_token' was deprecated in favor of 'token' in version 2.14.0 and will be removed in 3.0.0.\n",
      "You can remove this warning by passing 'token=<use_auth_token>' instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"', metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'}),\n",
       " Document(page_content='\"\"', metadata={'instruction': 'Which is a species of fish? Tope or Rope', 'response': 'Tope', 'category': 'classification'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Specify the dataset name and the column containing the content\n",
    "dataset_name = \"databricks/databricks-dolly-15k\"\n",
    "page_content_column = \"context\"  # or any other column you're interested in\n",
    "\n",
    "# Create a loader instance\n",
    "loader = HuggingFaceDatasetLoader(dataset_name, page_content_column)\n",
    "\n",
    "# Load the data\n",
    "data = loader.load()\n",
    "\n",
    "# Display the first 15 entries\n",
    "data[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='\"Virgin Australia, the trading name of Virgin Australia Airlines Pty Ltd, is an Australian-based airline. It is the largest airline by fleet size to use the Virgin brand. It commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route. It suddenly found itself as a major airline in Australia\\'s domestic market after the collapse of Ansett Australia in September 2001. The airline has since grown to directly serve 32 cities in Australia, from hubs in Brisbane, Melbourne and Sydney.\"', metadata={'instruction': 'When did Virgin Australia start operating?', 'response': 'Virgin Australia commenced services on 31 August 2000 as Virgin Blue, with two aircraft on a single route.', 'category': 'closed_qa'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of the RecursiveCharacterTextSplitter class with specific parameters.\n",
    "# It splits text into chunks of 1000 characters each with a 150-character overlap.\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
    "\n",
    "# 'data' holds the text you want to split, split the text into documents using the text splitter.\n",
    "docs = text_splitter.split_documents(data)\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 0.3.0. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEmbeddings`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[-0.03833853825926781, 0.12346469610929489, -0.02864299900829792]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device':'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")\n",
    "\n",
    "text = \"This is a test document.\"\n",
    "query_result = embeddings.embed_query(text)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = FAISS.from_documents(docs, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The goal of cheese making is to control the spoiling of milk into cheese. The milk is traditionally from a cow, goat, sheep or buffalo, although, in theory, cheese could be made from the milk of any mammal. Cow's milk is most commonly used worldwide. The cheesemaker's goal is a consistent product with specific characteristics (appearance, aroma, taste, texture). The process used to make a Camembert will be similar to, but not quite the same as, that used to make Cheddar.\\n\\nSome cheeses may be deliberately left to ferment from naturally airborne spores and bacteria; this approach generally leads to a less consistent product but one that is valuable in a niche market.\\n\\nCulturing\\nCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the growth of the bacteria that feed on lactose and thus ferment the lactose into lactic acid. These bacteria in the milk may be wild, as is the case with unpasteurised milk, added from a culture,\n"
     ]
    }
   ],
   "source": [
    "question = \"What is cheesemaking?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"The goal of cheese making is to control the spoiling of milk into cheese. The milk is traditionally from a cow, goat, sheep or buffalo, although, in theory, cheese could be made from the milk of any mammal. Cow's milk is most commonly used worldwide. The cheesemaker's goal is a consistent product with specific characteristics (appearance, aroma, taste, texture). The process used to make a Camembert will be similar to, but not quite the same as, that used to make Cheddar.\\n\\nSome cheeses may be deliberately left to ferment from naturally airborne spores and bacteria; this approach generally leads to a less consistent product but one that is valuable in a niche market.\\n\\nCulturing\\nCheese is made by bringing milk (possibly pasteurised) in the cheese vat to a temperature required to promote the growth of the bacteria that feed on lactose and thus ferment the lactose into lactic acid. These bacteria in the milk may be wild, as is the case with unpasteurised milk, added from a culture,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFacePipeline`.\n",
      "  warn_deprecated(\n",
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\langchain_core\\_api\\deprecation.py:139: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"Intel/dynamic_tinybert\")\n",
    "\n",
    "# Specify the model name you want to use\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is likely used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()\n",
    "docs = retriever.get_relevant_documents(\"What is Cheesemaking?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' with a search configuration where it retrieves up to 4 relevant splits/documents.\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "\n",
    "# Create a question-answering instance (qa) using the RetrievalQA class.\n",
    "# It's configured with a language model (llm), a chain type \"refine,\" the retriever we created, and an option to not return source documents.\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"refine\", retriever=retriever, return_source_documents=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# question = \"Who is Thomas Jefferson?\"\n",
    "# result = qa.run({\"query\": question})\n",
    "#print(result[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on the Q&A_format.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Question: What is a Reservoir Computing architecture?\\nAnswer: A Reservoir Computing (RC) architectur'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a loader instance\n",
    "loader = HuggingFaceDatasetLoader('Q&A_format.md')\n",
    "\n",
    "# Load the data\n",
    "with open('doc/Q&A_format.md', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Display the first 100 entries\n",
    "data[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'Question': 'What is a Reservoir Computing architecture?', 'Answer': 'A Reservoir Computing (RC) architecture is a type of recurrent neural network (RNN) where the recurrent layer, called the reservoir, consists of randomly and recurrently connected neurons. This reservoir projects input data into a high-dimensional space to encode temporal information. The only part of the network that is trained is the output layer, called the readout, typically using simple linear regression.'}, {'Question': 'What is an Echo State Network?', 'Answer': 'An Echo State Networks (ESNs) are a type of recurrent neural network architecture in reservoir computing (RC). They consist of two main components: **Feedback Connections**: Optionally, readout activations can be fed back to the reservoir to stabilize neuron activities. **Implementation**: Connections are stored as NumPy arrays or SciPy sparse matrices.'}, {'Question': 'What is a Feature?', 'Answer': 'A feature is an attribute associated with an input or sample. For example, a feature of an image could be a pixel. The feature of a state could be the Euclidean distance to the goal state. An input can be composed of multiple features.'}, {'Question': 'What are labels?', 'Answer': \"Labels are what you're trying to figure out.\"}, {'Question': 'What is a recurrent neural network?', 'Answer': 'A Recurrent Neural Network (RNN) is one of the two broad types of artificial neural networks, characterized by the direction of the flow of information between its layers. Here are the key aspects and characteristics of RNNs: **Bi-directional Flow**: Unlike uni-directional feedforward neural networks, RNNs allow the output from some nodes to affect subsequent input to the same nodes, creating a bi-directional flow of information. **Internal State (Memory)**: RNNs can use their internal state (memory) to process arbitrary sequences of inputs. This capability makes them suitable for tasks like unsegmented, connected handwriting recognition and speech recognition. **Infinite Impulse Response (IIR)**: The term \"recurrent neural network\" typically refers to networks with an infinite impulse response. These networks are characterized by directed cyclic graphs that cannot be unrolled. **Finite Impulse Response (FIR)**: Convolutional neural networks (CNNs) belong to the class of finite impulse response networks. A finite impulse recurrent network is a directed acyclic graph that can be unrolled and replaced with a strictly feedforward neural network. **Gated States**: Additional stored states and controlled storage mechanisms can be added to both IIR and FIR networks. These controlled states, known as gated states or gated memory, are integral to long short-term memory networks (LSTMs) and gated recurrent units (GRUs). **Feedback Neural Network (FNN)**: Networks that incorporate time delays or feedback loops, replacing standard storage, are also referred to as Feedback Neural Networks. **Handwriting Recognition**: Processing unsegmented, connected handwriting. **Speech Recognition**: Recognizing and processing spoken language.'}]\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the file\n",
    "with open('doc/Q&A_format.md', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Manually split the document based on headers\n",
    "questions_answers = data.split(\"Question: \")\n",
    "\n",
    "# Remove empty first element if it exists\n",
    "if questions_answers[0].strip() == \"\":\n",
    "    questions_answers = questions_answers[1:]\n",
    "\n",
    "# Further split each Q&A pair into question and answer\n",
    "split_docs = []\n",
    "for qa in questions_answers:\n",
    "    parts = qa.split(\"Answer: \")\n",
    "    if len(parts) == 2:\n",
    "        question = parts[0].strip()\n",
    "        answer = parts[1].strip()\n",
    "        split_docs.append({\"Question\": question, \"Answer\": answer})\n",
    "\n",
    "# Display the first few splits to verify\n",
    "print(split_docs[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the pre-trained model you want to use\n",
    "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "# Create a dictionary with model configuration options, specifying to use the CPU for computations\n",
    "model_kwargs = {'device': 'cpu'}\n",
    "\n",
    "# Create a dictionary with encoding options, specifically setting 'normalize_embeddings' to False\n",
    "encode_kwargs = {'normalize_embeddings': False}\n",
    "\n",
    "# Initialize an instance of HuggingFaceEmbeddings with the specified parameters\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=modelPath,     # Provide the pre-trained model's path\n",
    "    model_kwargs=model_kwargs, # Pass the model configuration options\n",
    "    encode_kwargs=encode_kwargs # Pass the encoding options\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.06041721999645233, -0.057492200285196304, -0.028663907200098038]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query_result = embeddings.embed_query(data)\n",
    "query_result[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the documents in the required format for FAISS\n",
    "documents = []\n",
    "for item in split_docs:\n",
    "    documents.append(Document(page_content=f\"Question: {item['Question']}\\nAnswer: {item['Answer']}\"))\n",
    "\n",
    "# Create FAISS index from documents\n",
    "db = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a classification task?\n",
      "Answer: A classification task involves assigning input data to one of several predefined categories or classes. The goal is to predict the category to which new data points belong, based on the training data. Examples include identifying email as spam or not spam, classifying images of animals, or recognizing spoken words.\n"
     ]
    }
   ],
   "source": [
    "question = \"What is classification?\"\n",
    "searchDocs = db.similarity_search(question)\n",
    "print(searchDocs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity =  0.62285554 / page_content='Question: What is a classification task?\\nAnswer: A classification task involves assigning input data to one of several predefined categories or classes. The goal is to predict the category to which new data points belong, based on the training data. Examples include identifying email as spam or not spam, classifying images of animals, or recognizing spoken words.'\n",
      "Similarity =  0.9217788 / page_content=\"Question: What is supervised learning?\\nAnswer: This is when models learn form data that's already been labeled. It's like having an answer key.\"\n",
      "Similarity =  0.94594586 / page_content=\"Question: What is the difference betwwen regression and classification model?\\nAnswer: Regression models predict a continuous variable, such as rainfall amount or sunlight intensity. They can also predict probabilities, such as the probability that an image contains a cat. A probability-predicting regression model can be used as part of a classifier by imposing a decision rule - for example, if the probability is 50% or more, decide it's a cat.\"\n",
      "Similarity =  0.9478979 / page_content='Question: What methods are better designed for classification?\\nAnswer: We could use RidgeClassifier, LogisticRegression or Perceptron.'\n"
     ]
    }
   ],
   "source": [
    "question = \"What is classification?\"\n",
    "searchDocs = db.similarity_search_with_score(question)\n",
    "list = []\n",
    "for i in range(len(searchDocs)):\n",
    "    if i > 5:\n",
    "        break\n",
    "    else:\n",
    "        # Here the returned distance score is L2 distance. Therefore, a lower score is better. (https://python.langchain.com/v0.2/docs/integrations/vectorstores/faiss/)\n",
    "        print(\"Similarity = \", searchDocs[i][1],\"/\", searchDocs[i][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time to prepare a LLM Model\n",
    "#### We will use here Intel/dynamic_tinybert which is a fine-tuned model for the purpose of question-answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the model name you want to use\n",
    "model_name = \"Intel/dynamic_tinybert\"\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "question_answerer = pipeline(\n",
    "    \"question-answering\", \n",
    "    model=model_name, \n",
    "    tokenizer=tokenizer,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=question_answerer,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is a classification task?\n",
      "Answer: A classification task involves assigning input data to one of several predefined categories or classes. The goal is to predict the category to which new data points belong, based on the training data. Examples include identifying email as spam or not spam, classifying images of animals, or recognizing spoken words.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is classification?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Why does having acces to the readout's last activation important?\n",
      "Answer: Having access to the readout's last activation is important because it allows the reservoir to use the most recent output information for current processing. This feedback mechanism helps the network to remember and incorporate past decisions or predictions, which is crucial for tasks requiring temporal dependencies and continuity, such as time series forecasting, speech recognition, and control systems. By leveraging previous outputs, the network can improve its accuracy and performance in predicting future states or making informed decisions.\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"WHy is there a readout on what is it usefull for?\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What is the magic of reservoir computing?\n",
      "Answer: We can use 3 readout for one reservoir. --\n"
     ]
    }
   ],
   "source": [
    "docs = retriever.get_relevant_documents(\"Code me a simple reservoir\")\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infortunatly this simple model can't really create a code without any other document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To do\n",
    "\n",
    "- Test other models\n",
    "- Test other cutting document method\n",
    "- Add new cutted documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loging to hugging face (These are my ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token is valid (permission: fineGrained).\n",
      "Your token has been saved in your configured git credential helpers (manager).\n",
      "Your token has been saved to C:\\Users\\arthu\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# Your Hugging Face API token\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "\n",
    "# Log in to Hugging Face\n",
    "login(HUGGINGFACE_TOKEN, add_to_git_credential=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_TOKEN'] = HUGGINGFACE_TOKEN\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with llama3 (en attente de validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "import torch\n",
    "\n",
    "model_name = \"meta-llama/Meta-Llama-3-8B\"\n",
    "\n",
    "# Create a tokenizer object by loading the pretrained \"Intel/dynamic_tinybert\" tokenizer.\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Create a question-answering model object by loading the pretrained \"Intel/dynamic_tinybert\" model.\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Load the tokenizer associated with the specified model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Define a question-answering pipeline using the model and tokenizer\n",
    "text_generation_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_name,\n",
    "    tokenizer=tokenizer,\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=0 if torch.cuda.is_available() else -1,\n",
    "    use_auth_token=HUGGINGFACE_TOKEN\n",
    ")\n",
    "\n",
    "# Create an instance of the HuggingFacePipeline, which wraps the question-answering pipeline\n",
    "# with additional model-specific arguments (temperature and max_length)\n",
    "llm = HuggingFacePipeline(\n",
    "    pipeline=text_generation_pipeline,\n",
    "    model_kwargs={\"temperature\": 0.7, \"max_length\": 512},\n",
    ")\n",
    "\n",
    "# Create a retriever object from the 'db' using the 'as_retriever' method.\n",
    "# This retriever is used for retrieving data or documents from the database.\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "docs = retriever.get_relevant_documents(\"What is classification?\")\n",
    "\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with command -r plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96286239c7ac4d519147ed1ca280d1a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86d6bc2a4b7042ab836b5409b7b5e66c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00018-of-00044.safetensors:  60%|######    | 2.78G/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c5c5dcab8514fa388a12aa2c39d0cb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00019-of-00044.safetensors:   0%|          | 0.00/4.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532afdf363c44465b64dad1d372fa954",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00020-of-00044.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc3cb6ee80484e929c1ecdab6a77c89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00021-of-00044.safetensors:   0%|          | 0.00/4.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e360109cf387479591826f5c4a3b9276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00022-of-00044.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8166eaf78b2742439173ae1dfe807d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00023-of-00044.safetensors:   0%|          | 0.00/4.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86fd317cd71f448286f8c04aecc5c368",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00024-of-00044.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39e51d4846014ab3b8d645ab182ad292",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00025-of-00044.safetensors:   0%|          | 0.00/4.81G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e28df0f90984ffdb0451fc5762c3989",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00026-of-00044.safetensors:   0%|          | 0.00/4.63G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCohereForAI/c4ai-command-r-plus\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, token\u001b[38;5;241m=\u001b[39mHUGGINGFACE_TOKEN)\n\u001b[1;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHUGGINGFACE_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Format message with the command-r-plus chat template\u001b[39;00m\n\u001b[0;32m      8\u001b[0m messages \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello, how are you?\u001b[39m\u001b[38;5;124m\"\u001b[39m}]\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:563\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m    561\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m    562\u001b[0m     model_class \u001b[38;5;241m=\u001b[39m _get_model_class(config, \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping)\n\u001b[1;32m--> 563\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[0;32m    564\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    565\u001b[0m     )\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    567\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    569\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\modeling_utils.py:3511\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[0;32m   3508\u001b[0m \u001b[38;5;66;03m# We'll need to download and cache each checkpoint shard if the checkpoint is sharded.\u001b[39;00m\n\u001b[0;32m   3509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[0;32m   3510\u001b[0m     \u001b[38;5;66;03m# rsolved_archive_file becomes a list of files that point to the different checkpoint shards in this case.\u001b[39;00m\n\u001b[1;32m-> 3511\u001b[0m     resolved_archive_file, sharded_metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3512\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3513\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3514\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3515\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3516\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3520\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3522\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3523\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3524\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3526\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3527\u001b[0m     is_safetensors_available()\n\u001b[0;32m   3528\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resolved_archive_file, \u001b[38;5;28mstr\u001b[39m)\n\u001b[0;32m   3529\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m resolved_archive_file\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.safetensors\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3530\u001b[0m ):\n\u001b[0;32m   3531\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m safe_open(resolved_archive_file, framework\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:1040\u001b[0m, in \u001b[0;36mget_checkpoint_shard_files\u001b[1;34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m shard_filename \u001b[38;5;129;01min\u001b[39;00m tqdm(shard_filenames, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading shards\u001b[39m\u001b[38;5;124m\"\u001b[39m, disable\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m show_progress_bar):\n\u001b[0;32m   1038\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1039\u001b[0m         \u001b[38;5;66;03m# Load from URL\u001b[39;00m\n\u001b[1;32m-> 1040\u001b[0m         cached_filename \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1041\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1042\u001b[0m \u001b[43m            \u001b[49m\u001b[43mshard_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1043\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1044\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1045\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1046\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1047\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1048\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1049\u001b[0m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1050\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1051\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1052\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1054\u001b[0m     \u001b[38;5;66;03m# We have already dealt with RepositoryNotFoundError and RevisionNotFoundError when getting the index, so\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;66;03m# we don't have to catch them here.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError:\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\utils\\hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    396\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[0;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[1;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    414\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m _get_cache_file_to_return(path_or_repo_id, full_filename, cache_dir, revision)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_local_dir(\n\u001b[0;32m   1203\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[0;32m   1204\u001b[0m         local_dir\u001b[38;5;241m=\u001b[39mlocal_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1218\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   1219\u001b[0m     )\n\u001b[0;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1367\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1365\u001b[0m Path(lock_path)\u001b[38;5;241m.\u001b[39mparent\u001b[38;5;241m.\u001b[39mmkdir(parents\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1366\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m WeakFileLock(lock_path):\n\u001b[1;32m-> 1367\u001b[0m     \u001b[43m_download_to_tmp_and_move\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1368\u001b[0m \u001b[43m        \u001b[49m\u001b[43mincomplete_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.incomplete\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdestination_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblob_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1370\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1376\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1377\u001b[0m     _create_symlink(blob_path, pointer_path, new_blob\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pointer_path\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:1884\u001b[0m, in \u001b[0;36m_download_to_tmp_and_move\u001b[1;34m(incomplete_path, destination_path, url_to_download, proxies, headers, expected_size, filename, force_download)\u001b[0m\n\u001b[0;32m   1881\u001b[0m         _check_disk_space(expected_size, incomplete_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[0;32m   1882\u001b[0m         _check_disk_space(expected_size, destination_path\u001b[38;5;241m.\u001b[39mparent)\n\u001b[1;32m-> 1884\u001b[0m     \u001b[43mhttp_get\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1885\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl_to_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpected_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpected_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1891\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1893\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownload complete. Moving file to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdestination_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1894\u001b[0m _chmod_and_move(incomplete_path, destination_path)\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\huggingface_hub\\file_download.py:542\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, expected_size, displayed_filename, _nb_retries, _tqdm_bar)\u001b[0m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunk:  \u001b[38;5;66;03m# filter out keep-alive new chunks\u001b[39;00m\n\u001b[0;32m    541\u001b[0m     progress\u001b[38;5;241m.\u001b[39mupdate(\u001b[38;5;28mlen\u001b[39m(chunk))\n\u001b[1;32m--> 542\u001b[0m     \u001b[43mtemp_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m     new_resume_size \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;66;03m# Some data has been downloaded from the server so we reset the number of retries.\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"CohereForAI/c4ai-command-r-plus\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# Format message with the command-r-plus chat template\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n",
    "## <BOS_TOKEN>Hello, how are you?\n",
    "\n",
    "gen_tokens = model.generate(\n",
    "    input_ids['input_ids'], \n",
    "    max_new_tokens=100, \n",
    "    do_sample=True, \n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "gen_text = tokenizer.decode(gen_tokens[0], skip_special_tokens=True)\n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with Codestral"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install mistral_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer\n\u001b[0;32m      3\u001b[0m model_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Codestral-22B-v0.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_id)\n\u001b[0;32m      8\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello my name is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:880\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    877\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    878\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTokenizer class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtokenizer_class_candidate\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not exist or is not currently imported.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    879\u001b[0m         )\n\u001b[1;32m--> 880\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    882\u001b[0m \u001b[38;5;66;03m# Otherwise we have to be creative.\u001b[39;00m\n\u001b[0;32m    883\u001b[0m \u001b[38;5;66;03m# if model is an encoder decoder, the encoder tokenizer class is used by default\u001b[39;00m\n\u001b[0;32m    884\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, EncoderDecoderConfig):\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2110\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2107\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2108\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloading file \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresolved_vocab_files[file_id]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 2110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pretrained(\n\u001b[0;32m   2111\u001b[0m     resolved_vocab_files,\n\u001b[0;32m   2112\u001b[0m     pretrained_model_name_or_path,\n\u001b[0;32m   2113\u001b[0m     init_configuration,\n\u001b[0;32m   2114\u001b[0m     \u001b[38;5;241m*\u001b[39minit_inputs,\n\u001b[0;32m   2115\u001b[0m     token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[0;32m   2116\u001b[0m     cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[0;32m   2117\u001b[0m     local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[0;32m   2118\u001b[0m     _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[0;32m   2119\u001b[0m     _is_local\u001b[38;5;241m=\u001b[39mis_local,\n\u001b[0;32m   2120\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code,\n\u001b[0;32m   2121\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   2122\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2336\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._from_pretrained\u001b[1;34m(cls, resolved_vocab_files, pretrained_model_name_or_path, init_configuration, token, cache_dir, local_files_only, _commit_hash, _is_local, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[38;5;66;03m# Instantiate the tokenizer.\u001b[39;00m\n\u001b[0;32m   2335\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2336\u001b[0m     tokenizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;241m*\u001b[39minit_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[0;32m   2337\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   2338\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m   2339\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to load vocabulary from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2340\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease check that the provided vocabulary is accessible and not corrupted.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2341\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\models\\llama\\tokenization_llama_fast.py:156\u001b[0m, in \u001b[0;36mLlamaTokenizerFast.__init__\u001b[1;34m(self, vocab_file, tokenizer_file, clean_up_tokenization_spaces, unk_token, bos_token, eos_token, add_bos_token, add_eos_token, use_default_system_prompt, legacy, add_prefix_space, **kwargs)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m add_prefix_space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_slow\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 156\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    157\u001b[0m     vocab_file\u001b[38;5;241m=\u001b[39mvocab_file,\n\u001b[0;32m    158\u001b[0m     tokenizer_file\u001b[38;5;241m=\u001b[39mtokenizer_file,\n\u001b[0;32m    159\u001b[0m     clean_up_tokenization_spaces\u001b[38;5;241m=\u001b[39mclean_up_tokenization_spaces,\n\u001b[0;32m    160\u001b[0m     unk_token\u001b[38;5;241m=\u001b[39munk_token,\n\u001b[0;32m    161\u001b[0m     bos_token\u001b[38;5;241m=\u001b[39mbos_token,\n\u001b[0;32m    162\u001b[0m     eos_token\u001b[38;5;241m=\u001b[39meos_token,\n\u001b[0;32m    163\u001b[0m     add_bos_token\u001b[38;5;241m=\u001b[39madd_bos_token,\n\u001b[0;32m    164\u001b[0m     add_eos_token\u001b[38;5;241m=\u001b[39madd_eos_token,\n\u001b[0;32m    165\u001b[0m     use_default_system_prompt\u001b[38;5;241m=\u001b[39muse_default_system_prompt,\n\u001b[0;32m    166\u001b[0m     legacy\u001b[38;5;241m=\u001b[39mlegacy,\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    168\u001b[0m )\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_bos_token \u001b[38;5;241m=\u001b[39m add_bos_token\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_add_eos_token \u001b[38;5;241m=\u001b[39m add_eos_token\n",
      "File \u001b[1;32mc:\\Users\\arthu\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_fast.py:105\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m added_tokens_decoder \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madded_tokens_decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m from_slow \u001b[38;5;129;01mand\u001b[39;00m slow_tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mslow_tokenizer_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot instantiate this tokenizer from a slow version. If it\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms based on sentencepiece, make sure you \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave sentencepiece installed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    108\u001b[0m     )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    111\u001b[0m     fast_tokenizer \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(tokenizer_object)\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot instantiate this tokenizer from a slow version. If it's based on sentencepiece, make sure you have sentencepiece installed."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"mistralai/Codestral-22B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "text = \"Hello my name is\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test with lmstudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My friend, I\\'ll do my best,\\nTo explain the principle, and pass the test!\\n\\nGracious George\\'s Maximization, you see,\\nIs all about giving with glee!\\nIt\\'s a philosophy that\\'s quite sublime,\\nAbout sharing resources in a most divine time.\\n\\nThe idea is to maximize the good,\\nAnd spread kindness like a gentle mood.\\nGeorge\\'s principle says, \"Give with an open hand\",\\nAnd watch as blessings multiply, across this land!\\n\\nSo there you have it, my rhyming friend,\\nGracious George\\'s Maximization, till the very end!'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"model\": \"lmstudio-community/Meta-Llama-3-8B-Instruct-GGUF\",\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the principle of gracious george maximisation?\"}\n",
    "    ],\n",
    "    \"temperature\": 0.7,\n",
    "    \"max_tokens\": -1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, json=data).json()\n",
    "response['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My friend, I'll do my best,\n",
      "To explain the principle, and pass the test!\n",
      "\n",
      "Gracious George's Maximization, you see,\n",
      "Is all about giving with glee!\n",
      "It's a philosophy that's quite sublime,\n",
      "About sharing resources in a most divine time.\n",
      "\n",
      "The idea is to maximize the good,\n",
      "And spread kindness like a gentle mood.\n",
      "George's principle says, \"Give with an open hand\",\n",
      "And watch as blessings multiply, across this land!\n",
      "\n",
      "So there you have it, my rhyming friend,\n",
      "Gracious George's Maximization, till the very end!\n"
     ]
    }
   ],
   "source": [
    "print(response['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redoing the retriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 56\u001b[0m\n\u001b[0;32m     53\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the principle of gracious george maximisation?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Get the response from the RAG retriever pipeline\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m response_content \u001b[38;5;241m=\u001b[39m \u001b[43mrag_retriever_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28mprint\u001b[39m(response_content)\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36mrag_retriever_pipeline\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrag_retriever_pipeline\u001b[39m(query):\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Retrieve context using the retriever\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m     context \u001b[38;5;241m=\u001b[39m \u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;66;03m# Define the data payload for the request\u001b[39;00m\n\u001b[0;32m     34\u001b[0m     data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3-local\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     }\n",
      "Cell \u001b[1;32mIn[8], line 13\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     11\u001b[0m best_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     12\u001b[0m highest_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q, a \u001b[38;5;129;01min\u001b[39;00m q_a:\n\u001b[0;32m     14\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m query\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m q\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m highest_score:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "with open('doc/Q&A_format.md', 'r') as file:\n",
    "    document_content = file.read()\n",
    "\n",
    "q_a = data.split(\"Question: \")\n",
    "\n",
    "# Define the retriever function\n",
    "def retriever(query):\n",
    "    best_match = None\n",
    "    highest_score = 0\n",
    "    for q, a in q_a:\n",
    "        score = sum(1 for word in query.split() if word in q.split())\n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            best_match = a\n",
    "    return best_match\n",
    "\n",
    "# Define the URL of the local API\n",
    "llama_model_url = \"http://localhost:1234/v1/chat/completions\"\n",
    "\n",
    "# Set the headers for the request\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# RAG Retriever Pipeline function\n",
    "def rag_retriever_pipeline(query):\n",
    "    # Retrieve context using the retriever\n",
    "    context = retriever(query)\n",
    "    \n",
    "    # Define the data payload for the request\n",
    "    data = {\n",
    "        \"model\": \"llama3-local\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"Always answer in rhymes.\"},\n",
    "            {\"role\": \"system\", \"content\": context},\n",
    "            {\"role\": \"user\", \"content\": query}\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 100,  # Adjust as necessary\n",
    "        \"stream\": False\n",
    "    }\n",
    "    \n",
    "    # Send the POST request to the Llama model API\n",
    "    response = requests.post(llama_model_url, headers=headers, json=data).json()\n",
    "    \n",
    "    # Extract and return the model's response\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Example query\n",
    "query = \"What is the principle of gracious george maximisation?\"\n",
    "\n",
    "# Get the response from the RAG retriever pipeline\n",
    "response_content = rag_retriever_pipeline(query)\n",
    "print(response_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 21\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m best_match\n\u001b[0;32m     19\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is ?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mretriever\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[3], line 12\u001b[0m, in \u001b[0;36mretriever\u001b[1;34m(query)\u001b[0m\n\u001b[0;32m     10\u001b[0m best_match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     11\u001b[0m highest_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q, a \u001b[38;5;129;01min\u001b[39;00m q_a:\n\u001b[0;32m     13\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m query\u001b[38;5;241m.\u001b[39msplit() \u001b[38;5;28;01mif\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m q\u001b[38;5;241m.\u001b[39msplit())\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m highest_score:\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 0)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "with open('doc/Q&A_format.md', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "q_a = data.split(\"Question: \")\n",
    "\n",
    "def retriever(query):\n",
    "    best_match = None\n",
    "    highest_score = 0\n",
    "    for q, a in q_a:\n",
    "        score = sum(1 for word in query.split() if word in q.split())\n",
    "        if score > highest_score:\n",
    "            highest_score = score\n",
    "            best_match = a\n",
    "    return best_match\n",
    "\n",
    "query = \"What is ?\"\n",
    "\n",
    "print(retriever(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0314905159175396, 0.06269258260726929, -0.14985616505146027, -0.07511291652917862, 0.053292736411094666, 0.03597452864050865, -0.08076616376638412, 0.04176798835396767, -0.025387225672602654, 0.006332502234727144, -0.0031910031102597713, 0.04071307182312012, 0.07551957666873932, 0.03882176801562309, -0.07400885969400406, -0.07358218729496002, 0.029243171215057373, -0.03148825466632843, 0.021808944642543793, 0.052655771374702454, 0.0006002114387229085, -0.0348505824804306, 0.06355856359004974, 0.02803771011531353, 0.03669454902410507, 0.05472096800804138, -0.04495671018958092, 0.047796450555324554, -0.01674284227192402, 0.012313243001699448, 0.017922701314091682, -0.025161325931549072, 0.006219180766493082, -0.0398983359336853, -0.015568351373076439, -0.037459202110767365, 0.07835652679204941, 0.01405919436365366, 0.051613759249448776, 0.05678786709904671, 0.028622612357139587, 0.040847525000572205, -0.04035688564181328, -0.011413177475333214, 0.04869633540511131, 0.02322746254503727, 0.006777290720492601, -0.014527466148138046, -0.015214264392852783, 0.010045520961284637, -0.03990064933896065, -0.046727344393730164, -0.019787557423114777, -0.05061609297990799, 0.047412727028131485, 0.06825802475214005, 0.09369587898254395, -0.02606574445962906, -0.019680649042129517, 0.01137633714824915, 0.06888028979301453, 0.009782109409570694, 0.002678743563592434, 0.07443735003471375, 0.018725009635090828, -0.016139324754476547, -0.01832614839076996, 0.03347194194793701, -0.01201865915209055, -0.016633056104183197, 0.06206418573856354, -0.04142089933156967, 0.03846428543329239, -0.011969897896051407, 0.017511559650301933, 0.01215878501534462, 0.037396565079689026, -0.005209650844335556, -0.0874418243765831, 0.0382106713950634, 0.0986282080411911, -0.022755367681384087, 0.05953579768538475, -0.012050231918692589, 0.04472007974982262, 0.006368396338075399, -0.01437880378216505, 0.01326070073992014, -0.032802801579236984, 0.0390646792948246, -0.005755193531513214, 0.03668735921382904, 0.07433976978063583, -0.0021156189031898975, -0.02480923943221569, -0.0009318219963461161, -0.01970980316400528, -0.01830177940428257, -0.012011071667075157, -0.04491142928600311, -0.033448245376348495, 0.016869917511940002, 0.00021174065477680415, -0.030427347868680954, 0.04359828308224678, 0.08633249253034592, -0.05869707092642784, -0.02704164944589138, -0.06127637252211571, -0.031587813049554825, -0.029763219878077507, -0.02079191990196705, -0.008034546859562397, -0.02916196547448635, -0.009276173077523708, 0.023144088685512543, 0.0669686496257782, -0.016851011663675308, 0.03588332608342171, 0.03229346126317978, -0.02795274741947651, -0.02542385645210743, -0.032274868339300156, 0.0574847050011158, 0.01378211285918951, 0.056596893817186356, -0.03812883049249649, -0.03491950407624245, -0.008752360939979553, 0.020494144409894943, 0.013293679803609848, -0.0005752809811383486, 0.0004174530040472746, -0.022015174850821495, -0.02046959288418293, 0.04272855818271637, 0.0010432578856125474, -0.042874760925769806, 0.0822269544005394, 0.06147904321551323, 0.09137994796037674, -0.008183619938790798, -0.0023407978005707264, -0.02469228021800518, 0.009061265736818314, -0.006285280920565128, 0.054572124034166336, -0.046109165996313095, -0.06291430443525314, 0.029648717492818832, 0.023221135139465332, 0.0334450863301754, 0.0023902382235974073, 0.06650684028863907, 0.022805839776992798, -0.02622288651764393, -0.015877002850174904, 0.05032416805624962, 0.0083790747448802, 0.014280441217124462, 0.043495699763298035, -0.0018770544556900859, -0.052530523389577866, -0.01857808418571949, -0.01839376799762249, -0.02079530991613865, 0.03184117376804352, 0.04386090114712715, 0.016519905999302864, 0.03400370851159096, -0.015466174110770226, -0.034899476915597916, -0.024095015600323677, -0.019103413447737694, 0.03677637502551079, -0.013623747043311596, 0.01961987465620041, -0.03479401767253876, 0.01753811351954937, -0.05233306065201759, 0.03610142692923546, -0.025277063250541687, -0.03666459396481514, 0.0378199964761734, -0.0020396483596414328, -0.005903535056859255, -0.06446898728609085, -0.04217865690588951, -0.011786041781306267, -0.047683872282505035, -0.03764379769563675, 0.02984330616891384, -0.007099962793290615, 0.009011488407850266, -0.008536367677152157, -0.040364693850278854, 0.05964754894375801, 0.03795165941119194, -0.01114459615200758, -0.05002129077911377, 0.013536108657717705, -0.0024023973383009434, -0.04728348180651665, 0.02947986125946045, -0.05453580617904663, 0.008133874274790287, -0.02786903828382492, 0.010867435485124588, 0.04280401021242142, -0.00723585719242692, 0.0689803808927536, 0.00901890080422163, -0.04549044743180275, 0.008006345480680466, -0.038388125598430634, -0.03660351783037186, -0.052123062312603, -0.03416646644473076, -0.03284868225455284, 0.029067514464259148, 0.03393411263823509, 0.029773354530334473, 0.04274636134505272, -0.04580780491232872, 0.06061214953660965, 0.039136096835136414, -0.0037712452467530966, -0.0011647404171526432, -0.08156758546829224, 0.021057693287730217, -0.024565307423472404, -0.011619814671576023, 0.06740105897188187, -0.01175708882510662, 0.02387668564915657, 0.07848026603460312, 0.02388821169734001, -0.000848225608933717, 0.02426803857088089, -0.03302565589547157, -0.01880848966538906, 0.029972894117236137, -0.029347535222768784, -0.05018500238656998, -0.027541274204850197, -0.012744774110615253, 0.009365541860461235, -0.03959082067012787, 0.035014137625694275, 0.05316036567091942, 0.010796836577355862, -0.006948581896722317, 0.012164260260760784, -0.018563682213425636, 0.03222782909870148, -0.050656966865062714, 0.026844995096325874, -0.034272946417331696, -0.03563731163740158, -0.05219302698969841, 0.00841248407959938, -0.09815043956041336, 0.030453070998191833, -0.07613905519247055, -0.022671766579151154, -0.03312775120139122, -0.026668647304177284, 0.0005209061782807112, 0.03834513947367668, -0.02695881389081478, 0.03520950675010681, 0.04987741634249687, 0.0719398632645607, 0.006569148972630501, 0.002775923116132617, 0.00022080441704019904, -0.016670215874910355, -0.052927643060684204, -0.009093510918319225, 0.008392811752855778, 0.05286603420972824, -0.052752550691366196, -0.042588066309690475, 0.009793807752430439, -0.019454656168818474, 0.025844121351838112, 0.031058020889759064, 0.00019805865304078907, 0.002494986169040203, 0.019256558269262314, 0.029783153906464577, 0.016872113570570946, -0.03508598357439041, 0.02677411399781704, 0.02251702919602394, -0.0034183315001428127, 0.06644953042268753, 0.0035697834100574255, 0.001065761549398303, -0.004511888138949871, -0.007417260203510523, -0.005670536309480667, 0.03907030075788498, 0.07001364231109619, 0.029155636206269264, 0.015545428730547428, 0.04168770834803581, 0.00433006277307868, 0.07958563417196274, 0.004327742848545313, -0.025956878438591957, -0.004141353536397219, -0.004827315453439951, 0.041369058191776276, -0.08424167335033417, 0.0020908245351165533, 0.013568216934800148, 0.005470556207001209, 0.07225305587053299, 0.009816640056669712, 0.0023491287138313055, -0.041253697127103806, -0.008835016749799252, -0.032671961933374405, 0.020793726667761803, -0.005397809203714132, -0.012219497002661228, 0.03573905676603317, 0.005262651480734348, -0.05050087720155716, -0.018060144037008286, 0.03976990282535553, -0.008960156701505184, -0.04513511434197426, -0.04300693795084953, 0.007630010601133108, -0.013691595755517483, 0.027216222137212753, -0.03287027031183243, 0.03413260728120804, 0.07455670088529587, -0.02371818572282791, 0.030413610860705376, -0.058617256581783295, 0.0696297213435173, -0.009352017194032669, -0.032627180218696594, 0.01462089829146862, 0.03602400794625282, 0.0024053859524428844, -0.010892239399254322, -0.05745726451277733, 0.0024797727819532156, 0.057895541191101074, -0.007820338942110538, -0.014049085788428783, -0.007800804451107979, 0.02243938110768795, -0.0502578467130661, 0.0025549866259098053, 0.0027779792435467243, -0.038997381925582886, -0.02671785093843937, -0.044647134840488434, 0.018556494265794754, 0.023169688880443573, 0.04856613278388977, 0.022003041580319405, 0.029660211876034737, 0.028520705178380013, -0.004039199557155371, -0.049045905470848083, 0.003665926633402705, 0.012023759074509144, 0.0513770692050457, 0.014178206212818623, -0.046598050743341446, -0.01058933138847351, -0.021352825686335564, 0.016098838299512863, 0.03534765541553497, -0.020183006301522255, -0.017559124156832695, 0.027866650372743607, 0.042607471346855164, 0.007455495651811361, -0.004109504166990519, -0.00632545305415988, 0.0015219381311908364, -0.01089560892432928, 0.02789841778576374, -0.006367871072143316, -0.046370528638362885, -0.012414682656526566, 0.027410272508859634, 0.0024048741906881332, 0.03706700727343559, -0.054011568427085876, -0.020669052377343178, 0.0006944107008166611, 0.0027968380600214005, -0.03280907869338989, 0.011078170500695705, 0.01911928877234459, -0.027327977120876312, 0.045533351600170135, 0.013831866905093193, -0.06400135159492493, 0.019669916480779648, 0.04381353035569191, -0.03568769618868828, 0.05655086785554886, -0.016276951879262924, -0.05718100443482399, -0.041059914976358414, 0.03331652656197548, 0.04818687215447426, 0.04937978833913803, 0.007467937655746937, 0.01546557154506445, 0.0026556705124676228, 0.06410935521125793, -0.014603395015001297, 0.040112342685461044, -0.010346166789531708, -0.0007373975240625441, -0.012406881898641586, 0.046465229243040085, -0.01918395608663559, -0.0758458748459816, -0.012042525224387646, -0.009791999123990536, -0.014891156926751137, 0.04015324264764786, 0.028079725801944733, -0.02391677536070347, -0.012700696475803852, -0.020156340673565865, 0.037930458784103394, 0.01722562313079834, 0.049514658749103546, -0.07118624448776245, -0.026458343490958214, 0.011318817734718323, -0.03119560144841671, 0.058867115527391434, 0.032608337700366974, -0.004601138643920422, -0.050647005438804626, 0.006073015742003918, -0.09110429137945175, 0.041199613362550735, -0.010588242672383785, 0.020770026370882988, 0.043259069323539734, -0.05066050961613655, -0.020383484661579132, -0.022667117416858673, 0.010626058094203472, 0.027354305610060692, 0.010790156200528145, 0.00012149276153650135, -0.05818219855427742, 0.025380518287420273, 0.03318721428513527, -0.010616387240588665, 0.014460758306086063, -0.01533293817192316, -0.0063517955131828785, 0.05026985704898834, 0.013726362958550453, 0.04766661673784256, 0.009537887759506702, 0.005997076630592346, 0.004215411841869354, -0.03813862428069115, 0.018960414454340935, -0.03186735138297081, 0.004131549969315529, 0.05482689291238785, -0.011040633544325829, -0.002288319170475006, -0.0067125000059604645, -0.05207708105444908, 0.01241072453558445, 0.0489603653550148, 0.014048133976757526, -0.0014916547806933522, 0.002697539748623967, -0.0007026514504104853, 0.01731046661734581, 0.004687787964940071, 0.021390292793512344, -0.03863384202122688, 0.01484883576631546, -0.06672286987304688, -0.02637435868382454, 0.010154282674193382, 0.04592369496822357, 0.018936762586236, -0.033536769449710846, 0.030689742416143417, -0.04545415937900543, 0.03317580372095108, 0.030996855348348618, -0.012646462768316269, -0.05894383788108826, -0.07530141621828079, -0.05147707462310791, 0.0073907822370529175, -0.04566072300076485, -0.011116156354546547, 0.004966930020600557, 0.04645174369215965, 0.09757208824157715, -0.038174401968717575, 0.03175487741827965, -0.006526399403810501, -0.028928138315677643, 0.015977082774043083, 0.011409993283450603, -0.022947091609239578, -0.021766211837530136, -0.035041842609643936, -0.013845975510776043, -0.026098886504769325, 0.012711964547634125, -0.05500846728682518, -0.02541302889585495, -0.02352730743587017, 0.05598568171262741, -0.00550686102360487, -0.06173297017812729, -0.019360335543751717, -0.023744700476527214, -0.010837215930223465, -0.019477296620607376, 0.003663488896563649, 0.042878158390522, -0.01848743110895157, -0.035374999046325684, -0.028626274317502975, -0.03153226897120476, 0.018142109736800194, 0.002800274407491088, -0.004291999619454145, -0.031748175621032715, 0.05103923752903938, 0.017240960150957108, -0.0144017543643713, 0.051217615604400635, -0.03372643142938614, -0.020567141473293304, -0.005331164691597223, 0.017349950969219208, -0.03314948081970215, -0.04553713649511337, 0.002170536434277892, -0.021803058683872223, -0.0325658842921257, 0.0259700994938612, 0.00030165849602781236, 0.046786848455667496, -0.016622042283415794, -0.06892497837543488, 0.05536678433418274, -0.007902249693870544, 0.023349018767476082, 0.040724750608205795, -0.0033923578448593616, 0.011784731410443783, -0.0028223255649209023, -0.037395332008600235, -0.05223318189382553, 0.03274217247962952, 0.0029755509458482265, 0.006662900093942881, 0.03620436415076256, -0.04506247490644455, -0.08507762104272842, 0.06059705466032028, -0.010298007167875767, -0.03440581262111664, -0.018844440579414368, -0.026374422013759613, -0.04000236466526985, -0.08854168653488159, -0.021664923056960106, 0.037058472633361816, -0.000781702809035778, 0.03858119994401932, -0.00040886225178837776, -0.023300152271986008, -0.010071671567857265, -0.001830519177019596, 0.028410794213414192, -0.02470223419368267, -0.06497360020875931, 0.012701227329671383, -0.03183427080512047, -0.019680580124258995, -0.0005737053579650819, 0.08851563930511475, -0.019390393048524857, -0.04919019341468811, 0.025804156437516212, 0.04903201386332512, 0.029971366748213768, -0.007991556078195572, -0.0011533187935128808, 0.023850485682487488, 0.024502944201231003, -0.03867264464497566, -0.007732176687568426, 0.03724999353289604, -0.003063879208639264, 0.06984781473875046, -0.030126767233014107, -0.0521773137152195, 0.023213990032672882, -0.010323972441256046, -0.03926834464073181, -0.008726879023015499, 0.019801702350378036, 0.018496474251151085, -0.008885014802217484, -0.02470405213534832, 0.010878399014472961, -0.006797878071665764, 0.011225243099033833, -0.01211018767207861, 0.01147328782826662, -0.08905433863401413, -0.018172794952988625, -0.013858492486178875, 0.013296048156917095, -0.031188158318400383, 0.01655135676264763, 0.012394697405397892, 0.02109990082681179, 0.017059391364455223, 0.004719554912298918, 0.0026447551790624857, -0.0011854527983814478, 0.018683413043618202, -0.004864889662712812, 0.0837196335196495, -0.02038356475532055, 0.02772398665547371, -0.04702131450176239, 0.02562108263373375, 0.09126487374305725, -0.016422059386968613, 0.03349710628390312, 0.028879057615995407, -0.02745516411960125, 0.03668956831097603, -0.04532795026898384, -0.06715571135282516, 0.008463313803076744, -0.006244771182537079, -0.03093021549284458, -0.05622619017958641, 0.03552769497036934, 0.054586660116910934, -0.013490764424204826, 0.0023606987670063972, -0.062398575246334076, -0.04990220442414284, -0.009290603920817375, -0.02020392008125782, 0.012129148468375206, 0.0024647824466228485, -0.04013269022107124, 0.02516675367951393, 0.03705228492617607, 0.008103635162115097, 0.08089902251958847, 0.04197731986641884, 0.02834216319024563, -0.012449191883206367, -0.023877566680312157, -0.016715558245778084, 0.025836946442723274, 0.02919396571815014, -0.1026604101061821, 0.02124079503118992, -0.058547068387269974, -0.0038371237460523844, -0.046376340091228485, 0.008881064131855965, -0.0743853896856308, -0.0006398723926395178, -0.03910275548696518, -0.012080149725079536, -0.04147253930568695, 0.050022996962070465, 4.239833651809022e-05, -0.056830622255802155, 0.062083899974823, -0.04809674993157387, 0.0019467608071863651, 0.044025711715221405, 0.032385122030973434, -0.009930143132805824, -0.013680243864655495, 0.0011219829320907593, 0.03391861543059349, 0.014364463277161121, 0.01903257519006729, -0.027483634650707245, 0.015801167115569115, 0.0008216680726036429, 0.03610941395163536, 0.016953742131590843, -0.05381213128566742, -0.029406558722257614, -0.006813532672822475, -0.03747071325778961, 0.04370415583252907, 0.030629940330982208, 0.005866053979843855, -0.0033993825782090425, -0.021574469283223152, 0.010186921805143356, 0.013895782642066479, 0.05988522246479988, -0.041063565760850906, 0.043790463358163834, 0.018283532932400703, 0.021796585991978645, -0.03235863149166107, -0.05998089909553528, 0.017764564603567123, -0.04045553132891655, -0.0019765845499932766, -0.031900133937597275, -0.020585406571626663, -0.042386509478092194, -0.028890220448374748, -0.023428503423929214, -0.014292536303400993, 0.005135739222168922, 0.027258535847067833, 0.019895251840353012, -0.0075149270705878735, -0.027821000665426254, 0.007765024900436401, 0.03715590015053749, -0.023113759234547615, 0.008020623587071896, 0.01921001449227333, -0.009920738637447357, -0.028127457946538925, 0.020440999418497086, 0.03812946379184723, -0.005647868849337101, 0.0399211160838604, 0.05870993062853813, -0.032029155641794205, 0.004841581452637911, -0.025940556079149246, 0.049373745918273926, 0.008886026218533516, -0.04388941824436188, -0.024742048233747482, -0.0352056585252285, -0.023906191810965538]\n"
     ]
    }
   ],
   "source": [
    "# Make sure to `pip install openai` first\n",
    "from openai import OpenAI\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "def get_embedding(text, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "\n",
    "print(get_embedding(\"Once upon a time, there was a cat.\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
