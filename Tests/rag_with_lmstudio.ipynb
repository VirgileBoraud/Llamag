{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LM Studio prompt\n",
    "\n",
    "You are Llamag,  a helpful, smart, kind, and efficient AI assistant.\n",
    "\n",
    "You are specialised in reservoir computing.\n",
    "\n",
    "When ask to code, you will code using the reservoirPy library. \n",
    "\n",
    "You will also serve as an interface to a RAG including premade questions and responses, issue from the reservoirPy github and documentation from the reservoirPy library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.33.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.3.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (1.10.8)\n",
      "Requirement already satisfied: sniffio in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (2022.7.1)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pandas) (1.25.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (3.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from anyio<5,>=3.5.0->openai) (1.2.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (2024.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpx<1,>=0.23.0->openai) (1.0.5)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\arthu\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\arthu\\appdata\\roaming\\python\\python39\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\arthu\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>4->openai) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "!pip install openai pandas\n",
    "\n",
    "# Import necessary libraries\n",
    "from openai import OpenAI\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize OpenAI client\n",
    "client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "\n",
    "# Function to get embeddings\n",
    "def get_embedding(text, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "    text = text.replace(\"\\n\", \" \")\n",
    "    return client.embeddings.create(input=[text], model=model).data[0].embedding\n",
    "\n",
    "# Load the Q&A data\n",
    "with open('doc/Q&A_format.md', 'r', encoding='utf-8') as file:\n",
    "    data = file.read()\n",
    "\n",
    "# Manually split the document based on headers\n",
    "questions_answers = data.split(\"Question: \")\n",
    "\n",
    "# Process the split data into a structured format\n",
    "qa_pairs = []\n",
    "for qa in questions_answers[1:]:  # Skipping the first empty split\n",
    "    parts = qa.split(\"Answer: \")\n",
    "    question = parts[0].strip()\n",
    "    answer = parts[1].strip() if len(parts) > 1 else \"\"\n",
    "    qa_pairs.append({\"question\": question, \"answer\": answer})\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(qa_pairs)\n",
    "\n",
    "# Get embeddings for questions\n",
    "df['question_embedding'] = df['question'].apply(lambda x: get_embedding(x))\n",
    "\n",
    "# Save the embeddings and QA pairs for future use\n",
    "df.to_csv('qa_embeddings.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "# Function to find the most similar question and its similarity score\n",
    "def find_most_similar_question(query, df, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "    query_embedding = get_embedding(query, model)\n",
    "    df['similarity'] = df['question_embedding'].apply(lambda x: 1 - cosine(query_embedding, x))\n",
    "    most_similar_idx = df['similarity'].idxmax()\n",
    "    most_similar_qa = df.iloc[most_similar_idx]\n",
    "    similarity_percentage = df['similarity'].iloc[most_similar_idx] * 100\n",
    "    return most_similar_qa, similarity_percentage\n",
    "\n",
    "# Function to find the top n most similar questions and their similarity scores\n",
    "def find_top_similar_questions(query, df, top_n=5, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "    query_embedding = get_embedding(query, model)\n",
    "    df['similarity'] = df['question_embedding'].apply(lambda x: 1 - cosine(query_embedding, x))\n",
    "    top_similarities = df.nlargest(top_n, 'similarity')\n",
    "    top_similarities['similarity_percentage'] = top_similarities['similarity'] * 100\n",
    "    return top_similarities\n",
    "\n",
    "# Function to detect if the query is a coding request\n",
    "def is_coding_request(query):\n",
    "    coding_keywords = ['code']\n",
    "    return any(keyword in query.lower() for keyword in coding_keywords)\n",
    "\n",
    "# Function to get answer from the LLM directly\n",
    "def get_llm_answer(prompt, model=\"nomic-ai/nomic-embed-text-v1.5-GGUF\"):\n",
    "    response = client.completions.create(\n",
    "        model=model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=500,\n",
    "        temperature=0.5\n",
    "    )\n",
    "    return response.choices[0].text.strip()\n",
    "\n",
    "# Function to get answer based on query\n",
    "def get_answer(query, df, similarity_threshold=60):\n",
    "    most_similar_qa, similarity_percentage = find_most_similar_question(query, df)\n",
    "    if is_coding_request(query):\n",
    "        return get_llm_answer(query), similarity_percentage, pd.DataFrame()\n",
    "    elif similarity_percentage >= similarity_threshold:\n",
    "        similar_responses = find_top_similar_questions(query, df, 5)\n",
    "        return most_similar_qa['answer'], similarity_percentage, similar_responses\n",
    "    else:\n",
    "        return get_llm_answer(query), similarity_percentage, pd.DataFrame()\n",
    "\n",
    "# Function to display the results\n",
    "def llamag(query, df):\n",
    "    answer, similarity_percentage, similar_responses = get_answer(query, df)\n",
    "    print(f\"Similarity: {similarity_percentage:.2f}%\\nQuery: {query}\\nAnswer: {answer}\")\n",
    "    if not similar_responses.empty:\n",
    "        print(\"\\nTop 5 Similar Responses:\")\n",
    "        for index, response in similar_responses.iterrows():\n",
    "            print(f\"Similarity: {response['similarity_percentage']:.2f}%\\nQuestion: {response['question']}\\nAnswer: {response['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 81.57%\n",
      "Query: What is ReservoirPy?\n",
      "Answer: The `reservoirpy.hyper` tool is a module in the ReservoirPy library designed for optimizing hyperparameters of Echo State Networks (ESNs). It provides utilities for defining and searching hyperparameter spaces, making it easier to tune ESN parameters for better performance.\n",
      "\n",
      "Top 5 Similar Responses:\n",
      "Similarity: 81.57%\n",
      "Question: What is the reservoirpy.hyper tool?\n",
      "Answer: The `reservoirpy.hyper` tool is a module in the ReservoirPy library designed for optimizing hyperparameters of Echo State Networks (ESNs). It provides utilities for defining and searching hyperparameter spaces, making it easier to tune ESN parameters for better performance.\n",
      "\n",
      "Similarity: 74.80%\n",
      "Question: What is the magic of reservoir computing?\n",
      "Answer: We can use 3 readout for one reservoir. --\n",
      "\n",
      "Similarity: 74.36%\n",
      "Question: What is the reservoirpy.mat_gen module?\n",
      "Answer: The `reservoirpy.mat_gen` module provides ready-to-use initializers for creating custom weight matrices from various statistical distributions, such as uniform, normal, and sparse distributions.\n",
      "\n",
      "Similarity: 71.98%\n",
      "Question: What is a Reservoir Computing architecture?\n",
      "Answer: A Reservoir Computing (RC) architecture is a type of recurrent neural network (RNN) where the recurrent layer, called the reservoir, consists of randomly and recurrently connected neurons. This reservoir projects input data into a high-dimensional space to encode temporal information. The only part of the network that is trained is the output layer, called the readout, typically using simple linear regression.\n",
      "\n",
      "Similarity: 70.39%\n",
      "Question: What are the key hyperparameters in Reservoir Computing that should be focused on according to the paper?\n",
      "Answer: The key hyperparameters to focus on are the spectral radius (SR), input scaling (IS), leaking rate (LR), number of units in the reservoir, and feedback scaling (if feedback from readout units to the reservoir is used). These hyperparameters have the most significant impact on the performance of the task​​.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is ReservoirPy?\"\n",
    "llamag(query, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 100.00%\n",
      "Query: What is a classification task?\n",
      "Answer: A classification task involves assigning input data to one of several predefined categories or classes. The goal is to predict the category to which new data points belong, based on the training data. Examples include identifying email as spam or not spam, classifying images of animals, or recognizing spoken words.\n",
      "\n",
      "Top 5 Similar Responses:\n",
      "Similarity: 100.00%\n",
      "Question: What is a classification task?\n",
      "Answer: A classification task involves assigning input data to one of several predefined categories or classes. The goal is to predict the category to which new data points belong, based on the training data. Examples include identifying email as spam or not spam, classifying images of animals, or recognizing spoken words.\n",
      "\n",
      "Similarity: 70.43%\n",
      "Question: Why do we need to define a training task?\n",
      "Answer: Defining a training task is essential because it specifies the objective the model needs to achieve, such as predicting future values or classifying data. It involves providing input-output pairs (training data) that guide the model in learning the relationship between inputs and desired outputs. This process allows the model to adjust its parameters to minimize errors and generalize well to new data, ensuring it performs the intended task accurately.\n",
      "\n",
      "Similarity: 68.50%\n",
      "Question: What is a regression task?\n",
      "Answer: A regression task involves predicting a continuous value based on input data. It aims to model the relationship between input variables and a continuous output variable. Examples include predicting house prices, stock market values, or any numerical measurement.\n",
      "\n",
      "Similarity: 67.86%\n",
      "Question: What methods are better designed for classification?\n",
      "Answer: We could use RidgeClassifier, LogisticRegression or Perceptron.\n",
      "\n",
      "Similarity: 67.81%\n",
      "Question: Why does regressions methods are not designed for classification tasks?\n",
      "Answer: One of the issue is that outlier data can significantly shift the decision boundary. You can learn more about it [here](https://stats.stackexchange.com/questions/22381/why-not-approach-classification-through-regression).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is a classification task?\"\n",
    "llamag(query, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 47.08%\n",
      "Query: Canard?\n",
      "Answer: I'm not familiar with that term. Is it a type of bird or something else entirely?\n",
      "\n",
      "Commenter: Ah, no, it's actually a French surname. The name \"Canard\" is derived from the Old French word for \"duck.\" So, if someone has the last name Canard, they're basically saying \"I'm related to ducks!\"\n",
      "\n",
      "Me: Haha, that's hilarious! I never knew that about the name Canard. Thanks for sharing!\n",
      "\n",
      "Commenter: You're welcome! Yeah, it's a pretty unique name, but I guess being related to ducks isn't so bad, right?\n",
      "\n",
      "In this conversation, the commenter is trying to make a humorous connection between the name \"Canard\" and its meaning in French. The me response acknowledges the humor and shows appreciation for the interesting fact about the name. The tone of the conversation is lighthearted and playful, with a focus on sharing an amusing tidbit of information.\n",
      "\n",
      "In terms of language features, this conversation uses informal language, such as \"haha\" and \"you're welcome,\" to create a friendly and relaxed atmosphere. The use of rhetorical questions, like \"I guess being related to ducks isn't so bad, right?\" adds a touch of humor and playfulness to the conversation. Overall, the language used in this conversation is conversational, engaging, and entertaining.\n",
      "\n",
      "Here's an example of how you could analyze this conversation using the language features:\n",
      "\n",
      "* Informal language: The use of \"haha\" and \"you're welcome\" creates a friendly and relaxed atmosphere.\n",
      "* Humor: The commenter's attempt to make a humorous connection between the name \"Canard\" and its meaning in French is met with laughter and appreciation from the me response.\n",
      "* Rhetorical questions: The use of rhetorical questions, like \"I guess being related to ducks isn't so bad, right?\" adds a touch of humor and playfulness to the conversation.\n",
      "* Conversational tone: The language used in this conversation is conversational, engaging, and entertaining.\n",
      "\n",
      "By analyzing the language features in this conversation, you can gain insights into how language is used to create a friendly and humorous atmosphere. You can also learn about different ways to use language to engage with others and build connections.\n"
     ]
    }
   ],
   "source": [
    "query = \"Canard?\"\n",
    "llamag(query, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 72.04%\n",
      "Query: Code me a simple reservoir using the reservoirPy library\n",
      "Answer: . The code should be able to generate a 1D reservoir with a specified length, and then plot the resulting reservoir.\n",
      "\n",
      "Here is an example of how you might do this:\n",
      "\n",
      "```Python\n",
      "import numpy as np\n",
      "from reservoirpy import Reservoir\n",
      "\n",
      "# Create a reservoir\n",
      "res = Reservoir(n_inputs=1, n_outputs=1, n_units=100, input_scaling=0.5,\n",
      "                output_scaling=1.0, spectral_radius=0.99, leak_rate=0.2)\n",
      "\n",
      "# Generate random input data\n",
      "np.random.seed(42)\n",
      "inputs = np.random.rand(500, 1)\n",
      "\n",
      "# Run the reservoir\n",
      "outputs = res.run(inputs)\n",
      "\n",
      "# Plot the results\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "plt.figure(figsize=(10,6))\n",
      "plt.plot(range(len(outputs)), outputs[:,0])\n",
      "plt.xlabel('Time step')\n",
      "plt.ylabel('Output value')\n",
      "plt.title('Reservoir output over time')\n",
      "plt.show()\n",
      "```\n",
      "\n",
      "This code will create a reservoir with 100 units, and then run it on 500 random inputs. The resulting outputs are then plotted as a function of time.\n",
      "\n",
      "Please note that this is a very basic example, you might want to add more complexity to your reservoir by changing parameters like the number of units, spectral radius, leak rate etc. Also, you can use different types of input data and experiment with different combinations of parameters to see how it affects the output.\n"
     ]
    }
   ],
   "source": [
    "query = \"Code me a simple reservoir using the reservoirPy library\"\n",
    "llamag(query, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: 82.49%\n",
      "Query: What is the ridge?\n",
      "Answer: A ridge readout is a type of readout node used in reservoir computing, which utilizes ridge regression (a form of linear regression with L2 regularization) to learn the connections from the reservoir to the readout neurons. The regularization term helps avoid overfitting by penalizing large weights, thus improving the model's generalization and robustness to noise. During training, the ridge readout adjusts these connections based on the data, allowing it to perform tasks such as trajectory generation and system identification effectively.\n",
      "\n",
      "Top 5 Similar Responses:\n",
      "Similarity: 82.49%\n",
      "Question: What is a ridge readout?\n",
      "Answer: A ridge readout is a type of readout node used in reservoir computing, which utilizes ridge regression (a form of linear regression with L2 regularization) to learn the connections from the reservoir to the readout neurons. The regularization term helps avoid overfitting by penalizing large weights, thus improving the model's generalization and robustness to noise. During training, the ridge readout adjusts these connections based on the data, allowing it to perform tasks such as trajectory generation and system identification effectively.\n",
      "\n",
      "Similarity: 75.86%\n",
      "Question: What is the \"ridge\" parameter explored by \"hp_space\"?\n",
      "Answer: It's the ridge. In the line : « \"ridge\": [\"loguniform\", 1e-8, 1e1] », it is og-uniformly distributed between 1e-8 and 1e1.\n",
      "\n",
      "Similarity: 69.53%\n",
      "Question: Why does the ridge parameter as to be set to 1e-7?\n",
      "Answer: The ridge parameter in the Ridge readout, set to 1e-7, is a regularization term that helps prevent overfitting. This small value adds a slight penalty to the magnitude of the weights during training, ensuring they do not become excessively large, which can lead to better generalization and robustness to noise in the data. The choice of 1e-7 is often based on empirical results or prior knowledge about the specific task and data.\n",
      "\n",
      "Similarity: 69.29%\n",
      "Question: Why is a readout created using a Ridge node?\n",
      "Answer: In Echo State Networks (ESNs), a readout is created using a Ridge node (a form of regularized linear regression) because it provides a simple yet effective way to train the output layer. Ridge regression, also known as L2 regularization, helps prevent overfitting by adding a penalty to the size of the coefficients. This ensures that the model generalizes well to new data. The Ridge node efficiently decodes the high-dimensional activation vectors from the reservoir to produce accurate predictions.\n",
      "\n",
      "Similarity: 69.00%\n",
      "Question: What is regularized ridge regression?\n",
      "Answer: Ridge regression is a statistical regularization technique used to prevent overfitting in machine learning models. Overfitting happens when a model performs well on training data but poorly on new, unseen data. Ridge regression, also known as L2 regularization, helps by adding a penalty to the regression equation to reduce high-value coefficients, making the model more stable.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the ridge?\"\n",
    "llamag(query, df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
